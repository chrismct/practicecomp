{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Home Credit Modeling\n",
        "### Chris McTeague\n",
        "### IS 6812\n",
        "### 11/3/2024"
      ],
      "metadata": {
        "id": "Yx6ohmXA3jOB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Business Problem Statement"
      ],
      "metadata": {
        "id": "u5YlRjS73734"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Many individuals with limited or no credit history struggle to obtain loans, often falling prey to unreliable lenders. Home Credit Group aims to broaden financial inclusion by providing safe loans to these individuals. This project will focus on improving Home Credit’s loan predictions of client’s repayment abilities. Effectively allowing them to reach more people without the risk of sky rocketing default rates."
      ],
      "metadata": {
        "id": "aUUQYZuE4GL-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install category_encoders\n",
        "!pip install xgboost\n",
        "!pip install hyperopt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9WU5zesAtdCC",
        "outputId": "ca8d71e0-ab4e-452c-f460-254e29986066",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting category_encoders\n",
            "  Downloading category_encoders-2.6.4-py2.py3-none-any.whl.metadata (8.0 kB)\n",
            "Requirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.10/dist-packages (from category_encoders) (1.26.4)\n",
            "Requirement already satisfied: scikit-learn>=0.20.0 in /usr/local/lib/python3.10/dist-packages (from category_encoders) (1.5.2)\n",
            "Requirement already satisfied: scipy>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from category_encoders) (1.13.1)\n",
            "Requirement already satisfied: statsmodels>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from category_encoders) (0.14.4)\n",
            "Requirement already satisfied: pandas>=1.0.5 in /usr/local/lib/python3.10/dist-packages (from category_encoders) (2.2.2)\n",
            "Requirement already satisfied: patsy>=0.5.1 in /usr/local/lib/python3.10/dist-packages (from category_encoders) (1.0.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.0.5->category_encoders) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.0.5->category_encoders) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.0.5->category_encoders) (2024.2)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.20.0->category_encoders) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.20.0->category_encoders) (3.5.0)\n",
            "Requirement already satisfied: packaging>=21.3 in /usr/local/lib/python3.10/dist-packages (from statsmodels>=0.9.0->category_encoders) (24.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas>=1.0.5->category_encoders) (1.16.0)\n",
            "Downloading category_encoders-2.6.4-py2.py3-none-any.whl (82 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m82.0/82.0 kB\u001b[0m \u001b[31m434.0 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: category_encoders\n",
            "Successfully installed category_encoders-2.6.4\n",
            "Requirement already satisfied: xgboost in /usr/local/lib/python3.10/dist-packages (2.1.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from xgboost) (1.26.4)\n",
            "Requirement already satisfied: nvidia-nccl-cu12 in /usr/local/lib/python3.10/dist-packages (from xgboost) (2.23.4)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from xgboost) (1.13.1)\n",
            "Requirement already satisfied: hyperopt in /usr/local/lib/python3.10/dist-packages (0.2.7)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from hyperopt) (1.26.4)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from hyperopt) (1.13.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from hyperopt) (1.16.0)\n",
            "Requirement already satisfied: networkx>=2.2 in /usr/local/lib/python3.10/dist-packages (from hyperopt) (3.4.2)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.10/dist-packages (from hyperopt) (1.0.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from hyperopt) (4.66.6)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.10/dist-packages (from hyperopt) (3.1.0)\n",
            "Requirement already satisfied: py4j in /usr/local/lib/python3.10/dist-packages (from hyperopt) (0.10.9.7)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VYNdW3wg0Vot"
      },
      "outputs": [],
      "source": [
        "#importing libraries\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import statsmodels.api as sm\n",
        "import category_encoders as ce\n",
        "import xgboost as xgb\n",
        "from scipy import stats\n",
        "from scipy.stats import zscore\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import cross_val_score, GridSearchCV, train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay,accuracy_score, precision_score, recall_score, f1_score, accuracy_score\n",
        "from sklearn.metrics import roc_auc_score, precision_recall_curve, confusion_matrix\n",
        "from sklearn.metrics import classification_report, average_precision_score\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from sklearn.utils import class_weight\n",
        "from sklearn.metrics import roc_curve, auc\n",
        "from scipy.stats import uniform, randint\n",
        "import xgboost as xgb\n",
        "from hyperopt import fmin, tpe, hp, STATUS_OK, Trials\n",
        "from sklearn.model_selection import train_test_split\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f0ckPJdJyO3d"
      },
      "source": [
        "# Data Cleaning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bmGBdk0Vx48c",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "#file paths\n",
        "train_data_path = \"/content/application_test.csv\"\n",
        "test_data_path = \"/content/application_test.csv\"\n",
        "\n",
        "#creating datasets\n",
        "train_data = pd.read_csv(\"application_train.csv\")\n",
        "test_data = pd.read_csv(\"application_test.csv\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Upon reviewing the dataset, we notice that several columns represent the same information but are calculated using different statistical measures, such as the average, mode, or median. To prevent multicollinearity and simplify the model, we will retain only the columns with average calculations. This will ensure we avoid redundant features while still capturing the key trends in the data."
      ],
      "metadata": {
        "id": "yUXhrI1rAEod"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#droping the given columns\n",
        "train_data = train_data.drop(columns = ['APARTMENTS_MODE','BASEMENTAREA_MODE','YEARS_BEGINEXPLUATATION_MODE','YEARS_BUILD_MODE','COMMONAREA_MODE','ELEVATORS_MODE',\n",
        "                                        'ENTRANCES_MODE','FLOORSMAX_MODE','FLOORSMIN_MODE','LANDAREA_MODE','LIVINGAPARTMENTS_MODE','LIVINGAREA_MODE','NONLIVINGAPARTMENTS_MODE',\n",
        "                                        'NONLIVINGAREA_MODE','APARTMENTS_MEDI','BASEMENTAREA_MEDI','YEARS_BEGINEXPLUATATION_MEDI','YEARS_BUILD_MEDI','COMMONAREA_MEDI',\n",
        "                                        'ELEVATORS_MEDI','ENTRANCES_MEDI','FLOORSMAX_MEDI','FLOORSMIN_MEDI','LANDAREA_MEDI','LIVINGAPARTMENTS_MEDI','LIVINGAREA_MEDI',\n",
        "                                        'NONLIVINGAPARTMENTS_MEDI','NONLIVINGAREA_MEDI'])\n",
        "\n",
        "test_data = test_data.drop(columns = ['APARTMENTS_MODE','BASEMENTAREA_MODE','YEARS_BEGINEXPLUATATION_MODE','YEARS_BUILD_MODE','COMMONAREA_MODE','ELEVATORS_MODE',\n",
        "                                        'ENTRANCES_MODE','FLOORSMAX_MODE','FLOORSMIN_MODE','LANDAREA_MODE','LIVINGAPARTMENTS_MODE','LIVINGAREA_MODE','NONLIVINGAPARTMENTS_MODE',\n",
        "                                        'NONLIVINGAREA_MODE','APARTMENTS_MEDI','BASEMENTAREA_MEDI','YEARS_BEGINEXPLUATATION_MEDI','YEARS_BUILD_MEDI','COMMONAREA_MEDI',\n",
        "                                        'ELEVATORS_MEDI','ENTRANCES_MEDI','FLOORSMAX_MEDI','FLOORSMIN_MEDI','LANDAREA_MEDI','LIVINGAPARTMENTS_MEDI','LIVINGAREA_MEDI',\n",
        "                                        'NONLIVINGAPARTMENTS_MEDI','NONLIVINGAREA_MEDI'])"
      ],
      "metadata": {
        "id": "yFHrtsNE_-d3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "At this stage, we are removing data points that are extreme outliers or appear to be clear typographical errors. For instance, one of the fields, \"Days Employed,\" contained values exceeding 365,243, which is over 1,000 years—an obvious anomaly. By eliminating such erroneous data, we aim to improve the overall quality of the dataset and ensure that our model isn't skewed by these incorrect entries."
      ],
      "metadata": {
        "id": "_qB0AEs9AF6t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#removing extreme outliers and potential errors that we noticed from the dataset\n",
        "train_data = train_data[train_data['DAYS_EMPLOYED'] != 365243]\n",
        "train_data = train_data[train_data['OBS_30_CNT_SOCIAL_CIRCLE'] != 348]\n",
        "train_data = train_data[train_data['DEF_30_CNT_SOCIAL_CIRCLE'] != 34]\n",
        "train_data = train_data[train_data['OBS_60_CNT_SOCIAL_CIRCLE'] != 344]\n",
        "train_data = train_data[train_data['DEF_60_CNT_SOCIAL_CIRCLE'] != 24]\n",
        "train_data = train_data[train_data['AMT_REQ_CREDIT_BUREAU_QRT'] != 264]\n",
        "train_data = train_data[train_data['AMT_REQ_CREDIT_BUREAU_QRT'] != 19]"
      ],
      "metadata": {
        "id": "V4gHW2-eAFZU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Below we are just binning some of our variables."
      ],
      "metadata": {
        "id": "YoNY-k6ZALB8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#replace NAs in AMT_ANNUITY with the column mean\n",
        "train_data['AMT_ANNUITY'].fillna(train_data['AMT_ANNUITY'].mean(), inplace=True)\n",
        "test_data['AMT_ANNUITY'].fillna(test_data['AMT_ANNUITY'].mean(), inplace=True)\n",
        "\n",
        "#replace NAs in AMT_GOODS_PRICE with the column mean\n",
        "train_data['AMT_GOODS_PRICE'].fillna(train_data['AMT_GOODS_PRICE'].mean(), inplace=True)\n",
        "test_data['AMT_GOODS_PRICE'].fillna(test_data['AMT_GOODS_PRICE'].mean(), inplace=True)\n",
        "\n",
        "#binning DAYS_BIRTH (convert to age in years)\n",
        "train_data['DAYS_BIRTH'] = abs(train_data['DAYS_BIRTH']) / 365\n",
        "test_data['DAYS_BIRTH'] = abs(test_data['DAYS_BIRTH']) / 365\n",
        "\n",
        "#binning ages into categories\n",
        "age_bins = [18, 25, 35, 45, 60, np.inf]\n",
        "age_labels = ['18-25', '26-35', '36-45', '46-60', '60+']\n",
        "train_data['DAYS_BIRTH'] = pd.cut(train_data['DAYS_BIRTH'], bins=age_bins, labels=age_labels, right=False)\n",
        "test_data['DAYS_BIRTH'] = pd.cut(test_data['DAYS_BIRTH'], bins=age_bins, labels=age_labels, right=False)\n",
        "\n",
        "#binning AMT_INCOME_TOTAL into categories\n",
        "income_bins = [0, 50000, 100000, 150000, 300000, np.inf]\n",
        "income_labels = ['Low income', 'Lower-middle income', 'Middle income', 'Upper-middle income', 'High income']\n",
        "train_data['AMT_INCOME_TOTAL'] = pd.cut(train_data['AMT_INCOME_TOTAL'], bins=income_bins, labels=income_labels, right=False)\n",
        "test_data['AMT_INCOME_TOTAL'] = pd.cut(test_data['AMT_INCOME_TOTAL'], bins=income_bins, labels=income_labels, right=False)\n",
        "\n",
        "#binning AMT_CREDIT into categories\n",
        "credit_bins = [0, 100000, 500000, 1000000, np.inf]\n",
        "credit_labels = ['Small loans', 'Medium loans', 'Large loans', 'Very large loans']\n",
        "train_data['AMT_CREDIT'] = pd.cut(train_data['AMT_CREDIT'], bins=credit_bins, labels=credit_labels, right=False)\n",
        "test_data['AMT_CREDIT'] = pd.cut(test_data['AMT_CREDIT'], bins=credit_bins, labels=credit_labels, right=False)\n",
        "\n",
        "#binning CNT_CHILDREN into categories\n",
        "children_bins = [0, 1, 2, 3, np.inf]\n",
        "children_labels = ['Single', '2 family members', '3 family members', '4 or more family members']\n",
        "train_data['CNT_CHILDREN'] = pd.cut(train_data['CNT_CHILDREN'], bins=children_bins, labels=children_labels, right=False)\n",
        "test_data['CNT_CHILDREN'] = pd.cut(test_data['CNT_CHILDREN'], bins=children_bins, labels=children_labels, right=False)"
      ],
      "metadata": {
        "id": "jji6gcbVALUe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fdfc69b9-673d-433c-afcc-e3f644127f43"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-87-3ed8c8fff49e>:2: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  train_data['AMT_ANNUITY'].fillna(train_data['AMT_ANNUITY'].mean(), inplace=True)\n",
            "<ipython-input-87-3ed8c8fff49e>:3: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  test_data['AMT_ANNUITY'].fillna(test_data['AMT_ANNUITY'].mean(), inplace=True)\n",
            "<ipython-input-87-3ed8c8fff49e>:6: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  train_data['AMT_GOODS_PRICE'].fillna(train_data['AMT_GOODS_PRICE'].mean(), inplace=True)\n",
            "<ipython-input-87-3ed8c8fff49e>:7: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  test_data['AMT_GOODS_PRICE'].fillna(test_data['AMT_GOODS_PRICE'].mean(), inplace=True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As the final step in our data cleaning process, we are applying a few last adjustments. We will convert boolean values into 1s and 0s for consistency. For missing values in numeric columns, we'll replace them with the column's mean to maintain the integrity of the data. For missing values in categorical columns, we'll fill them with \"XNA\" as a placeholder. This placeholder will not have any significant effect on the models but will ensure that no data is missing when we proceed with the analysis."
      ],
      "metadata": {
        "id": "OVpyEANmAU31"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#replacing binary variables with 1s and 0s\n",
        "train_data['FLAG_OWN_REALTY'] = train_data['FLAG_OWN_REALTY'].replace({'Y': 1, 'N': 0})\n",
        "\n",
        "train_data['FLAG_OWN_CAR'] = train_data['FLAG_OWN_CAR'].replace({'Y': 1, 'N': 0})\n",
        "\n",
        "#filling in numeric columns that have NAs with the mean of that column\n",
        "numeric_cols = train_data.select_dtypes(include=np.number).columns\n",
        "train_data[numeric_cols] = train_data[numeric_cols].fillna(train_data[numeric_cols].mean())\n",
        "\n",
        "columns_to_fill = ['NAME_TYPE_SUITE', 'OCCUPATION_TYPE', 'FONDKAPREMONT_MODE',\n",
        "                   'HOUSETYPE_MODE', 'WALLSMATERIAL_MODE', 'EMERGENCYSTATE_MODE']\n",
        "\n",
        "#filling categorical columns with 'XNA' for blank values\n",
        "train_data[columns_to_fill] = train_data[columns_to_fill].fillna('XNA')\n",
        "\n",
        "#reseting the index\n",
        "train_data = train_data.reset_index(drop=True)"
      ],
      "metadata": {
        "id": "THW59TgCAUmQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0403cce5-8ac5-4b6e-99b1-8366d06d834c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-88-98b1c780792e>:2: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
            "  train_data['FLAG_OWN_REALTY'] = train_data['FLAG_OWN_REALTY'].replace({'Y': 1, 'N': 0})\n",
            "<ipython-input-88-98b1c780792e>:4: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
            "  train_data['FLAG_OWN_CAR'] = train_data['FLAG_OWN_CAR'].replace({'Y': 1, 'N': 0})\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iNnKKGbYyWlt"
      },
      "source": [
        "# Data Processing"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "To prepare our data for use in machine learning models, we need to encode the categorical variables appropriately. We employ three different encoding techniques:\n",
        "\n",
        "One-Hot Encoding: This is used for nominal variables (those without a natural order) to create binary columns for each category.\n",
        "\n",
        "Label Encoding: Applied to ordinal variables, where the categories have an inherent order, converting them into integer labels that preserve this ranking.\n",
        "T\n",
        "arget Encoding: This is used for nominal variables with a high cardinality (i.e., too many unique categories). In this case, we replace each category with the mean value of the target variable for that category.\n",
        "\n",
        "By applying these encoding techniques, we ensure our categorical data is represented in a format suitable for different machine learning models, while also reducing dimensionality where necessary."
      ],
      "metadata": {
        "id": "p_2NGRvtAbgm"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NkyCNhqH6Elu"
      },
      "outputs": [],
      "source": [
        "le = LabelEncoder()\n",
        "train_encoded = train_data\n",
        "\n",
        "train_encoded = pd.get_dummies(train_data, columns=['CODE_GENDER', 'NAME_CONTRACT_TYPE'], drop_first=True)\n",
        "\n",
        "train_encoded['CNT_CHILDREN'] = le.fit_transform(train_data['CNT_CHILDREN'])\n",
        "train_encoded['AMT_INCOME_TOTAL'] = le.fit_transform(train_data['AMT_INCOME_TOTAL'])\n",
        "train_encoded['AMT_CREDIT'] = le.fit_transform(train_data['AMT_CREDIT'])\n",
        "train_encoded['DAYS_BIRTH'] = le.fit_transform(train_data['DAYS_BIRTH'])\n",
        "train_encoded['NAME_EDUCATION_TYPE'] = le.fit_transform(train_data['NAME_EDUCATION_TYPE'])\n",
        "train_encoded['WEEKDAY_APPR_PROCESS_START'] = le.fit_transform(train_data['WEEKDAY_APPR_PROCESS_START'])\n",
        "\n",
        "columns_to_encode = ['OCCUPATION_TYPE', 'NAME_HOUSING_TYPE','NAME_FAMILY_STATUS','EMERGENCYSTATE_MODE','WALLSMATERIAL_MODE','HOUSETYPE_MODE',\n",
        "                     'FONDKAPREMONT_MODE','ORGANIZATION_TYPE','NAME_INCOME_TYPE','NAME_TYPE_SUITE',]\n",
        "encoder = ce.TargetEncoder(cols=columns_to_encode)\n",
        "\n",
        "train_encoded = train_encoded.drop(columns=columns_to_encode)\n",
        "\n",
        "df_encoded = encoder.fit_transform(train_data[columns_to_encode], train_data['TARGET'])\n",
        "\n",
        "train_encoded = pd.concat([df_encoded, train_encoded], axis=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qJ642EFBzMY5"
      },
      "source": [
        "# ML Implementation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wu71NI1KqbCx"
      },
      "outputs": [],
      "source": [
        "#creating test and training sets of our features and target variable\n",
        "X = train_encoded.drop(columns=['TARGET','SK_ID_CURR'])\n",
        "y = train_encoded['TARGET']\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U-Sc4cLnuapH"
      },
      "source": [
        "## XGBoost"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# format data set appopriately for XGBoost modeling\n",
        "weights_train = np.where(y_train == 1, 10, 1)\n",
        "dtrain = xgb.DMatrix(X_train, label=y_train, weight = weights_train)\n",
        "dtest = xgb.DMatrix(X_test, label=y_test)"
      ],
      "metadata": {
        "id": "_DpP67FlXI9R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# define custom function\n",
        "def custom_cost_sensitive_eval(preds, dtrain):\n",
        "    labels = dtrain.get_label()\n",
        "    cost_false_negative = 5.0\n",
        "    cost_false_positive = 1.0\n",
        "    preds_binary = (preds >= 0.5).astype(int)\n",
        "\n",
        "    false_negatives = np.sum((labels == 1) & (preds_binary == 0))\n",
        "    false_positives = np.sum((labels == 0) & (preds_binary == 1))\n",
        "\n",
        "    custom_cost = (cost_false_negative * false_negatives) + (cost_false_positive * false_positives)\n",
        "    return \"cost_sensitive_error\", custom_cost, False"
      ],
      "metadata": {
        "id": "NbvL-n6lVrE0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# define hyperparams that will be tested\n",
        "param_dist = {\n",
        "    'max_depth': hp.randint('max_depth', 3, 10),\n",
        "    'learning_rate': hp.uniform('learning_rate', 0.01, 0.3),\n",
        "    'n_estimators': hp.randint('n_estimators', 50, 500),\n",
        "    'subsample': hp.uniform('subsample', 0.5, 1.0),\n",
        "    'colsample_bytree': hp.uniform('colsample_bytree', 0.5, 1.0),\n",
        "    'gamma': hp.uniform('gamma', 0, 5),\n",
        "    'min_child_weight': hp.randint('min_child_weight', 1, 10),\n",
        "}\n"
      ],
      "metadata": {
        "id": "QSNhYkCIXIAi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# define custome function\n",
        "def objective_function(params):\n",
        "    # Convert the sampled params to a dictionary compatible with XGBoost\n",
        "    xgb_params = {\n",
        "        'max_depth': int(params['max_depth']),\n",
        "        'learning_rate': params['learning_rate'],\n",
        "        'subsample': params['subsample'],\n",
        "        'colsample_bytree': params['colsample_bytree'],\n",
        "        'gamma': params['gamma'],\n",
        "        'min_child_weight': int(params['min_child_weight']),\n",
        "        'objective': 'binary:logistic',\n",
        "        'eval_metric': 'auc',\n",
        "    }\n",
        "\n",
        "    # Create DMatrix for training\n",
        "    weights_train = np.where(y_train == 1, 9, 1)\n",
        "    dtrain = xgb.DMatrix(X_train, label=y_train, weight=weights_train)\n",
        "\n",
        "    # Train the model\n",
        "    model = xgb.train(xgb_params, dtrain, num_boost_round=100)\n",
        "\n",
        "    # Evaluate on the training set for validation purposes\n",
        "    preds = model.predict(dtrain)\n",
        "\n",
        "    # Calculate custom cost-sensitive error\n",
        "    cost_sensitive_error = custom_cost_sensitive_eval(preds, dtrain)[1]\n",
        "\n",
        "    return {'loss': cost_sensitive_error, 'status': STATUS_OK}"
      ],
      "metadata": {
        "id": "YlkecJ82aXqJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#trials = Trials()\n",
        "#best_hyperparams = fmin(\n",
        "    #fn=objective_function,\n",
        "    #space=param_dist,\n",
        "    #algo=tpe.suggest,    # commented out for purpose of knitting final HTML, results shown below.\n",
        "    #max_evals=100,\n",
        "    #trials=trials\n",
        "#)\n",
        "\n",
        "#print(\"Best Hyperparameters:\", best_hyperparams)"
      ],
      "metadata": {
        "id": "Px2SmBUSaZiK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Best Hyperparameters: {'colsample_bytree': 0.9698674464932832, 'gamma': 1.6376259636304558, 'learning_rate': 0.299172816420403, 'max_depth': 9, 'min_child_weight': 1, 'n_estimators': 390, 'subsample': 0.9592765023912387}"
      ],
      "metadata": {
        "id": "ZEUeINh5qUGh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# hard code final param set for knitting purposes\n",
        "final_xgb_params = {\n",
        "    'max_depth': int(9),\n",
        "    'learning_rate': 0.299172816420403,\n",
        "    'subsample': 0.9592765023912387,\n",
        "    'colsample_bytree': 0.9698674464932832,\n",
        "    'gamma': 1.6376259636304558,\n",
        "    'min_child_weight': int(1),\n",
        "    'objective': 'binary:logistic',\n",
        "    'eval_metric': 'auc',\n",
        "}"
      ],
      "metadata": {
        "id": "GF76l0ITadfl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create test set\n",
        "weights_train = np.where(y_train == 1, 10, 1)\n",
        "dtrain_final = xgb.DMatrix(X_train, label=y_train, weight=weights_train)\n",
        "\n",
        "# Fit the final model\n",
        "final_model1 = xgb.train(final_xgb_params, dtrain_final, num_boost_round=100)\n",
        "\n",
        "# Evaluation on Test Set\n",
        "dtest_final = xgb.DMatrix(X_test, label=y_test)\n",
        "preds_final1 = final_model1.predict(dtest_final)"
      ],
      "metadata": {
        "id": "eTWFdRZGePjR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the recall, precision, and thresholds for different decision boundaries\n",
        "precision, recall, thresholds = precision_recall_curve(y_test, preds_final1)\n",
        "\n",
        "# Define desired recall\n",
        "desired_recall = 0.7\n",
        "\n",
        "# Find the threshold that produces the closest recall to the desired recall\n",
        "closest_idx = np.argmin(np.abs(recall - desired_recall))\n",
        "optimal_threshold = thresholds[closest_idx]\n",
        "\n",
        "print(f\"Optimal Threshold for Recall {desired_recall}: {optimal_threshold:.2f}\")\n",
        "\n",
        "# Use this threshold to make new predictions\n",
        "preds_custom_binary1 = (preds_final1 >= optimal_threshold).astype(int)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "idKwc5NPolVc",
        "outputId": "1179f3e5-461a-4922-df9e-b65b88cdb71b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Optimal Threshold for Recall 0.7: 0.22\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# create confusion matrix for results\n",
        "conf_matrix_custom = confusion_matrix(y_test, preds_custom_binary1)\n",
        "print(\"Confusion Matrix with Custom Threshold:\")\n",
        "print(conf_matrix_custom)\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy_custom = accuracy_score(y_test, preds_custom_binary1)\n",
        "print(f\"Accuracy with Custom Threshold: {accuracy_custom:.4f}\")\n",
        "\n",
        "# Get classification report for precision, recall, and F1 score\n",
        "class_report_custom = classification_report(y_test, preds_custom_binary1, target_names=['Majority Class', 'Minority Class'])\n",
        "print(\"Classification Report with Custom Threshold:\")\n",
        "print(class_report_custom)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s3wZpfN0pV1y",
        "outputId": "2e99fb3f-a1a7-4d35-898d-e39b278c5425"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Confusion Matrix with Custom Threshold:\n",
            "[[27566 18545]\n",
            " [ 1295  3021]]\n",
            "Accuracy with Custom Threshold: 0.6066\n",
            "Classification Report with Custom Threshold:\n",
            "                precision    recall  f1-score   support\n",
            "\n",
            "Majority Class       0.96      0.60      0.74     46111\n",
            "Minority Class       0.14      0.70      0.23      4316\n",
            "\n",
            "      accuracy                           0.61     50427\n",
            "     macro avg       0.55      0.65      0.48     50427\n",
            "  weighted avg       0.89      0.61      0.69     50427\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## XGBoost with SMOTE"
      ],
      "metadata": {
        "id": "ijQRtryd8w8W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# intiate SMOTE for unbalanced data\n",
        "smote = SMOTE(random_state=42)\n",
        "X_resampled, y_resampled = smote.fit_resample(X_train, y_train)\n",
        "\n",
        "# create new data frame for SMOTE data\n",
        "dtrain_resampled = xgb.DMatrix(X_resampled, label=y_resampled)\n",
        "\n",
        "#intiate SMOTE model\n",
        "model_SMOTE = xgb.train(final_xgb_params, dtrain_resampled, num_boost_round=100)\n",
        "\n",
        "# create test set\n",
        "dtest = xgb.DMatrix(X_test, label=y_test)\n",
        "\n",
        "# make predictions using SMOTE model\n",
        "preds_final_SMOTE = model_SMOTE.predict(dtest)\n",
        "\n",
        "# convert preds to binary as target is binary\n",
        "preds_binary_SMOTE = (preds_final >= .46).astype(int)\n",
        "\n",
        "# create matrix of results\n",
        "cm = confusion_matrix(y_test, preds_binary_SMOTE)\n",
        "report = classification_report(y_test, preds_binary_SMOTE)\n",
        "\n",
        "preds_final_SMOTE_proba = model_SMOTE.predict(dtest)\n",
        "\n",
        "fpr, tpr, thresholds = roc_curve(y_test, preds_final_SMOTE_proba)\n",
        "\n",
        "\n",
        "roc_auc = auc(fpr, tpr)\n",
        "\n",
        "\n",
        "print(f'ROC AUC: {roc_auc:.4f}')\n",
        "\n",
        "\n",
        "print(\"Confusion Matrix:\")\n",
        "print(cm)\n",
        "print(\"\\nClassification Report:\")\n",
        "print(report)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "swGCuyAmtwc-",
        "outputId": "3b9a37b0-af79-46f9-afc0-b519548e0c49"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ROC AUC: 0.7193\n",
            "Confusion Matrix:\n",
            "[[37986  8125]\n",
            " [ 2383  1933]]\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.94      0.82      0.88     46111\n",
            "           1       0.19      0.45      0.27      4316\n",
            "\n",
            "    accuracy                           0.79     50427\n",
            "   macro avg       0.57      0.64      0.57     50427\n",
            "weighted avg       0.88      0.79      0.83     50427\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## XGBoost Classifier"
      ],
      "metadata": {
        "id": "Ko1hdt4b811E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# initialize XGBoost Model\n",
        "#xgb_model = xgb.XGBClassifier(objective='binary:logistic', eval_metric='auc')\n",
        "\n",
        "#param_dist = {\n",
        "   # 'n_estimators': [300,400,500],\n",
        "    #'max_depth': [5,6,7],\n",
        "    #'learning_rate': [0.005, 0.05, 0.01, 0.1, 0.2],\n",
        "    #'subsample': [0.6, 0.8, 1.0,0.5,0.7],\n",
        "    #'colsample_bytree': [0.6, 0.8, 1.0],\n",
        "    #'gamma': [0, 0.1, 0.2],\n",
        "    #'min_child_weight': [1, 2, 3],\n",
        "#}\n",
        "\n",
        "# Set up random parameter search\n",
        "#random_search = RandomizedSearchCV(\n",
        "   # estimator=xgb_model,\n",
        "    #param_distributions=param_dist,      # commented out for purpose of knitting final HTML, results shown below.\n",
        "    #n_iter=50,\n",
        "   # scoring=['roc_auc', 'recall'],\n",
        "   # refit = 'roc_auc',\n",
        "   # cv=3,\n",
        "   # verbose=1,\n",
        "   # n_jobs=-1,\n",
        "   # random_state=42\n",
        "#)\n",
        "\n",
        "# Fit the model using DataFrame (not DMatrix)\n",
        "#random_search.fit(X_train, y_train)\n",
        "\n",
        "# Print results\n",
        "#print(\"Best Parameters:\", random_search.best_params_)\n",
        "#print(\"Best Score:\", random_search.best_score_)"
      ],
      "metadata": {
        "id": "ilgr5FHf3D0y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Best Hyperparameters: {'subsample': 0.8, 'n_estimators': 400, 'min_child_weight': 1, 'max_depth': 7, 'learning_rate': 0.01, 'gamma': 0, 'colsample_bytree': 0.8}\n",
        "Best Score: 0.966553380305374"
      ],
      "metadata": {
        "id": "1zQOBmI1UJ_p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# create data set for analysis\n",
        "X = train_encoded.drop(columns=['TARGET','SK_ID_CURR'])\n",
        "y = train_encoded['TARGET']\n",
        "weights_train = np.where(y_train == 1, 10, 1)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "weights_train = np.where(y_train == 1, 10, 1)\n",
        "\n",
        "# hard code best set of params for knitting purposes\n",
        "best_params = {\n",
        "    'objective': 'binary:logistic',\n",
        "    'eval_metric': 'auc',\n",
        "    'subsample': 0.8,\n",
        "    'n_estimators': 400,\n",
        "    'min_child_weight': 1,\n",
        "    'max_depth': 7,\n",
        "    'learning_rate': 0.01,\n",
        "    'gamma': 0,\n",
        "    'colsample_bytree': 0.8\n",
        "}\n",
        "\n",
        "# Create an XGBoost classifier with the best parameters\n",
        "xgb_classifier = xgb.XGBClassifier(**best_params)\n",
        "\n",
        "# Fit the model on the original training data\n",
        "xgb_classifier.fit(X_train, y_train, sample_weight=weights_train)\n",
        "\n",
        "# Generate predictions on the test set\n",
        "y_classifier_preds = xgb_classifier.predict(X_test)\n",
        "\n",
        "# Calculate and print accuracy\n",
        "accuracy_score_classifier = accuracy_score(y_test, y_classifier_preds)\n",
        "print(\"Test Accuracy:\", accuracy_score_classifier)\n",
        "\n",
        "# Generate and print the classification report\n",
        "print(\"Classification Report:\\n\", classification_report(y_test, y_classifier_preds))\n",
        "\n",
        "# Generate and print the confusion matrix\n",
        "conf_matrix = confusion_matrix(y_test, y_classifier_preds)\n",
        "print(\"Confusion Matrix:\\n\", conf_matrix)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NM6qoX8C490M",
        "outputId": "99f8fbb1-1fdc-45b3-c81a-9f65748d7972"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy: 0.7216372181569397\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.95      0.73      0.83     46111\n",
            "           1       0.18      0.63      0.28      4316\n",
            "\n",
            "    accuracy                           0.72     50427\n",
            "   macro avg       0.57      0.68      0.55     50427\n",
            "weighted avg       0.89      0.72      0.78     50427\n",
            "\n",
            "Confusion Matrix:\n",
            " [[33669 12442]\n",
            " [ 1595  2721]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hWzhGsxFzbQU"
      },
      "source": [
        "# Feature Expansion and Testing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XeyBbU8EzlOg"
      },
      "outputs": [],
      "source": [
        "#aggregating Bureau\n",
        "bureau = pd.read_csv(\"bureau.csv\")\n",
        "bureau_agg = bureau.groupby('SK_ID_CURR').agg({'DAYS_CREDIT': 'mean', 'CREDIT_DAY_OVERDUE': 'mean', 'CNT_CREDIT_PROLONG' : 'mean',\n",
        "                                               'AMT_CREDIT_SUM' : 'mean','AMT_CREDIT_SUM_DEBT' : 'mean','AMT_CREDIT_SUM_OVERDUE' : 'mean'})\n",
        "pd.DataFrame(bureau_agg)\n",
        "bureau_agg = bureau_agg.rename(columns={'DAYS_CREDIT':'AVG_DAYS_CREDIT', 'CREDIT_DAY_OVERDUE':'AVG_CREDIT_DAY_OVERDUE','CNT_CREDIT_PROLONG' :'AVG_CNT_CREDIT_PROLONG',\n",
        "                                        'AMT_CREDIT_SUM':'AVG_AMT_CREDIT_SUM', 'AMT_CREDIT_SUM_DEBT':'AVG_AMT_CREDIT_SUM_DEBT', 'AMT_CREDIT_SUM_OVERDUE':'AVG_AMT_CREDIT_SUM_OVERDUE'})\n",
        "\n",
        "train_bureau_encoded = pd.merge(train_encoded, bureau_agg, how='left', on='SK_ID_CURR')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# impute NA values\n",
        "numeric_cols = train_bureau_encoded.select_dtypes(include=np.number).columns\n",
        "\n",
        "train_bureau_encoded[numeric_cols] = train_bureau_encoded[numeric_cols].fillna(train_bureau_encoded[numeric_cols].mean())"
      ],
      "metadata": {
        "id": "kKTzdyuzsITm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create train and test sets with new improved data set\n",
        "X = train_bureau_encoded.drop(columns=['TARGET','SK_ID_CURR'])\n",
        "y = train_bureau_encoded['TARGET']\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n"
      ],
      "metadata": {
        "id": "CCmO66bls2tv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## XGBoost with Feature Expansion"
      ],
      "metadata": {
        "id": "mLbqf2NWS7ig"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# format train and test for XGBoost\n",
        "weights_train = np.where(y_train == 1, 10, 1)\n",
        "dtrain = xgb.DMatrix(X_train, label=y_train, weight = weights_train)\n",
        "dtest = xgb.DMatrix(X_test, label=y_test)"
      ],
      "metadata": {
        "id": "Zqr8eiS5S6ih"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# define function to apply unequal weight to false pos and false negs\n",
        "def custom_cost_sensitive_eval(preds, dtrain):\n",
        "    labels = dtrain.get_label()\n",
        "    cost_false_negative = 5.0\n",
        "    cost_false_positive = 1.0\n",
        "    preds_binary = (preds >= 0.5).astype(int)\n",
        "\n",
        "    false_negatives = np.sum((labels == 1) & (preds_binary == 0))\n",
        "    false_positives = np.sum((labels == 0) & (preds_binary == 1))\n",
        "\n",
        "    custom_cost = (cost_false_negative * false_negatives) + (cost_false_positive * false_positives)\n",
        "    return \"cost_sensitive_error\", custom_cost, False"
      ],
      "metadata": {
        "id": "k8svDvif74Ji"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# define parameters to test\n",
        "param_dist = {\n",
        "    'max_depth': hp.randint('max_depth', 3, 10),\n",
        "    'learning_rate': hp.uniform('learning_rate', 0.01, 0.3),\n",
        "    'n_estimators': hp.randint('n_estimators', 50, 500),\n",
        "    'subsample': hp.uniform('subsample', 0.5, 1.0),\n",
        "    'colsample_bytree': hp.uniform('colsample_bytree', 0.5, 1.0),\n",
        "    'gamma': hp.uniform('gamma', 0, 5),\n",
        "    'min_child_weight': hp.randint('min_child_weight', 1, 10),\n",
        "}"
      ],
      "metadata": {
        "id": "7ONpy8fV78ny"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# define objective function to save best set\n",
        "def objective_function(params):\n",
        "    xgb_params = {\n",
        "        'max_depth': int(params['max_depth']),\n",
        "        'learning_rate': params['learning_rate'],\n",
        "        'subsample': params['subsample'],\n",
        "        'colsample_bytree': params['colsample_bytree'],\n",
        "        'gamma': params['gamma'],\n",
        "        'min_child_weight': int(params['min_child_weight']),\n",
        "        'objective': 'binary:logistic',\n",
        "        'eval_metric': 'auc',\n",
        "    }\n",
        "\n",
        "    # Create DMatrix for training\n",
        "    weights_train = np.where(y_train == 1, 9, 1)\n",
        "    dtrain = xgb.DMatrix(X_train, label=y_train, weight=weights_train)\n",
        "\n",
        "    # Train the model\n",
        "    model = xgb.train(xgb_params, dtrain, num_boost_round=100)\n",
        "\n",
        "    # Evaluate on the training set for validation purposes\n",
        "    preds = model.predict(dtrain)\n",
        "\n",
        "    # Calculate custom cost-sensitive error\n",
        "    cost_sensitive_error = custom_cost_sensitive_eval(preds, dtrain)[1]\n",
        "\n",
        "    return {'loss': cost_sensitive_error, 'status': STATUS_OK}"
      ],
      "metadata": {
        "id": "lvNMfZ7H8FFL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#trials = Trials()\n",
        "#best_hyperparams = fmin(\n",
        "    #fn=objective_function,\n",
        "    #space=param_dist,          # commented out for purpose of knitting final HTML, results shown below.\n",
        "    #algo=tpe.suggest,\n",
        "    #max_evals=100,\n",
        "    #trials=trials\n",
        "#)\n",
        "\n",
        "#print(\"Best Hyperparameters:\", best_hyperparams)"
      ],
      "metadata": {
        "id": "mvbIcCIf8GiZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " Best Hyperparameters: {'colsample_bytree': 0.8771449965782033, 'gamma': 0.4993425025409346, 'learning_rate': 0.29480470259015035, 'max_depth': 9, 'min_child_weight': 2, 'n_estimators': 377, 'subsample': 0.9024966949013682}"
      ],
      "metadata": {
        "id": "3nE3OeZPEXGY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# hardcode final params for knitting\n",
        "final_xgb_params = {\n",
        "    'max_depth': int(9),\n",
        "    'learning_rate': 0.29480470259015035,\n",
        "    'subsample': 0.9024966949013682,\n",
        "    'colsample_bytree':  0.8771449965782033,\n",
        "    'gamma': 0.4993425025409346,\n",
        "    'min_child_weight': int(2),\n",
        "    'objective': 'binary:logistic',\n",
        "    'eval_metric': 'auc',\n",
        "}"
      ],
      "metadata": {
        "id": "k7sPRyRWEKkU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create final train set\n",
        "weights_train = np.where(y_train == 1, 10, 1)\n",
        "dtrain_final = xgb.DMatrix(X_train, label=y_train, weight=weights_train)\n",
        "\n",
        "# Fit the final model\n",
        "final_model = xgb.train(final_xgb_params, dtrain_final, num_boost_round=100)\n",
        "\n",
        "# Evaluation on Test Set\n",
        "dtest_final = xgb.DMatrix(X_test, label=y_test)\n",
        "preds_final = final_model.predict(dtest_final)"
      ],
      "metadata": {
        "id": "kfSqlYL4EkJi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the recall, precision, and thresholds for different decision boundaries\n",
        "precision, recall, thresholds = precision_recall_curve(y_test, preds_final)\n",
        "\n",
        "# Define desired recall\n",
        "desired_recall = 0.75\n",
        "\n",
        "# Find the threshold that produces the closest recall to the desired recall\n",
        "closest_idx = np.argmin(np.abs(recall - desired_recall))\n",
        "optimal_threshold = thresholds[closest_idx]\n",
        "\n",
        "print(f\"Optimal Threshold for Recall {desired_recall}: {optimal_threshold:.2f}\")\n",
        "\n",
        "# Use this threshold to make new predictions\n",
        "preds_custom_binary = (preds_final >= optimal_threshold).astype(int)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QB8LVuTnEpLT",
        "outputId": "801644c3-ed3a-4990-8f32-138b91b778a5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Optimal Threshold for Recall 0.75: 0.19\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# create conf matrix to display results\n",
        "conf_matrix_custom = confusion_matrix(y_test, preds_custom_binary)\n",
        "print(\"Confusion Matrix with Custom Threshold:\")\n",
        "print(conf_matrix_custom)\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy_custom = accuracy_score(y_test, preds_custom_binary)\n",
        "print(f\"Accuracy with Custom Threshold: {accuracy_custom:.4f}\")\n",
        "\n",
        "# Get classification report for precision, recall, and F1 score\n",
        "class_report_custom = classification_report(y_test, preds_custom_binary, target_names=['Majority Class', 'Minority Class'])\n",
        "print(\"Classification Report with Custom Threshold:\")\n",
        "print(class_report_custom)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SpwdnECbExFH",
        "outputId": "0b53802c-090b-4121-cf04-378b90765e9f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Confusion Matrix with Custom Threshold:\n",
            "[[25042 21069]\n",
            " [ 1079  3237]]\n",
            "Accuracy with Custom Threshold: 0.5608\n",
            "Classification Report with Custom Threshold:\n",
            "                precision    recall  f1-score   support\n",
            "\n",
            "Majority Class       0.96      0.54      0.69     46111\n",
            "Minority Class       0.13      0.75      0.23      4316\n",
            "\n",
            "      accuracy                           0.56     50427\n",
            "     macro avg       0.55      0.65      0.46     50427\n",
            "  weighted avg       0.89      0.56      0.65     50427\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## SMOTE XGBoost with Feature Expansion"
      ],
      "metadata": {
        "id": "CUJTMZ2bUPLV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# initiate SMOTE\n",
        "smote = SMOTE(random_state=42)\n",
        "# new train set\n",
        "X_resampled, y_resampled = smote.fit_resample(X_train, y_train)\n",
        "# format for XGBoost\n",
        "dtrain_resampled = xgb.DMatrix(X_resampled, label=y_resampled)\n",
        "# train model\n",
        "model_SMOTE = xgb.train(final_xgb_params, dtrain_resampled, num_boost_round=100)\n",
        "# new test set\n",
        "dtest = xgb.DMatrix(X_test, label=y_test)\n",
        "# make predictions\n",
        "preds_final_SMOTE = model_SMOTE.predict(dtest)\n",
        "# turn preds binary\n",
        "preds_binary_SMOTE = (preds_final >= .46).astype(int)\n",
        "# generate confusion matrix to display results\n",
        "cm = confusion_matrix(y_test, preds_binary_SMOTE)\n",
        "report = classification_report(y_test, preds_binary_SMOTE)\n",
        "\n",
        "preds_final_SMOTE_proba = model_SMOTE.predict(dtest)\n",
        "\n",
        "fpr, tpr, thresholds = roc_curve(y_test, preds_final_SMOTE_proba)\n",
        "\n",
        "\n",
        "roc_auc = auc(fpr, tpr)\n",
        "\n",
        "\n",
        "print(f'ROC AUC: {roc_auc:.4f}')\n",
        "\n",
        "\n",
        "print(\"Confusion Matrix:\")\n",
        "print(cm)\n",
        "print(\"\\nClassification Report:\")\n",
        "print(report)\n"
      ],
      "metadata": {
        "id": "RPQEkGl1UR21",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "27c9ddfc-008b-4a46-cb3b-c2a2b0d52f92"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ROC AUC: 0.7190\n",
            "Confusion Matrix:\n",
            "[[37986  8125]\n",
            " [ 2383  1933]]\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.94      0.82      0.88     46111\n",
            "           1       0.19      0.45      0.27      4316\n",
            "\n",
            "    accuracy                           0.79     50427\n",
            "   macro avg       0.57      0.64      0.57     50427\n",
            "weighted avg       0.88      0.79      0.83     50427\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## XGBClassifier with Feature Expansion"
      ],
      "metadata": {
        "id": "gqAZLOgYUXty"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# initialize XGBoost Model\n",
        "#xgb_model = xgb.XGBClassifier(objective='binary:logistic', eval_metric='auc')\n",
        "\n",
        "#param_dist = {\n",
        "    #'n_estimators': [300,400,500],\n",
        "    #'max_depth': [5,6,7],\n",
        "    #'learning_rate': [0.005, 0.05, 0.01, 0.1, 0.2],\n",
        "    #'subsample': [0.6, 0.8, 1.0,0.5,0.7],\n",
        "    #'colsample_bytree': [0.6, 0.8, 1.0],\n",
        "    #'gamma': [0, 0.1, 0.2],\n",
        "    #'min_child_weight': [1, 2, 3],\n",
        "#}\n",
        "\n",
        "# Set up random parameter search\n",
        "#random_search = RandomizedSearchCV(\n",
        "    #estimator=xgb_model,\n",
        "    #param_distributions=param_dist,\n",
        "    #n_iter=50,\n",
        "    #scoring=['roc_auc', 'recall'],   # chunk commented out for final HTML knot\n",
        "    #refit = 'roc_auc',\n",
        "    #cv=3,\n",
        "    #verbose=1,\n",
        "    #n_jobs=-1,\n",
        "    #random_state=42\n",
        "#)\n",
        "\n",
        "# Fit the model using DataFrame (not DMatrix)\n",
        "#random_search.fit(X_train, y_train)\n",
        "\n",
        "# Print results\n",
        "#print(\"Best Parameters:\", random_search.best_params_)\n",
        "#print(\"Best Score:\", random_search.best_score_)"
      ],
      "metadata": {
        "id": "vR08eXP_UilA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " Best Parameters: {'subsample': 0.7, 'n_estimators': 400, 'min_child_weight': 1, 'max_depth': 6, 'learning_rate': 0.05, 'gamma': 0, 'colsample_bytree': 0.6}"
      ],
      "metadata": {
        "id": "0pPL9km9WLXh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# hard code best set of params\n",
        "best_params = {\n",
        "    'objective': 'binary:logistic',\n",
        "    'eval_metric': 'auc',\n",
        "    'subsample': 0.7,\n",
        "    'n_estimators': 400,\n",
        "    'min_child_weight': 1,\n",
        "    'max_depth': 6,\n",
        "    'learning_rate': 0.05,\n",
        "    'gamma': 0,\n",
        "    'colsample_bytree': 0.6\n",
        "}\n",
        "\n",
        "# Create an XGBoost classifier with the best parameters\n",
        "xgb_classifier = xgb.XGBClassifier(**best_params)\n",
        "\n",
        "# Fit the model on the original training data\n",
        "xgb_classifier.fit(X_train, y_train, sample_weight=weights_train)\n",
        "\n",
        "# Generate predictions on the test set\n",
        "y_classifier_preds = xgb_classifier.predict(X_test)\n",
        "\n",
        "# Calculate and print accuracy\n",
        "accuracy_score_classifier = accuracy_score(y_test, y_classifier_preds)\n",
        "print(\"Test Accuracy:\", accuracy_score_classifier)\n",
        "\n",
        "# Generate and print the classification report\n",
        "print(\"Classification Report:\\n\", classification_report(y_test, y_classifier_preds))\n",
        "\n",
        "# Generate and print the confusion matrix\n",
        "conf_matrix = confusion_matrix(y_test, y_classifier_preds)\n",
        "print(\"Confusion Matrix:\\n\", conf_matrix)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DVYAzCkXWQti",
        "outputId": "cba5e1ea-6647-4fed-eb33-6336067997f4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy: 0.7406944692327523\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.95      0.75      0.84     46111\n",
            "           1       0.19      0.61      0.29      4316\n",
            "\n",
            "    accuracy                           0.74     50427\n",
            "   macro avg       0.57      0.68      0.56     50427\n",
            "weighted avg       0.89      0.74      0.79     50427\n",
            "\n",
            "Confusion Matrix:\n",
            " [[34729 11382]\n",
            " [ 1694  2622]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Kaggle Submission Test Data"
      ],
      "metadata": {
        "id": "ShPCUf6hXu9l"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4feVXs5xznh5"
      },
      "source": [
        "# Conclusions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E0EU_sebzqHq"
      },
      "source": [
        "In conclusion, despite implementing numerous modeling methods to attempt to accurately identify whether a customer will default or not, it does not appear that any model will be superior in terms of true accuracy of predictions than a simple majority classifier, which comes out to be around 91%. The dataset is extremely imbalanced, with instances of the majority class being much larger than those of the minority class, which is clearly weighing on our models. To account for that, we also implemented multiple methods to weight the dataset in an attempt to help train the models to predict the minority class with more accuracy, but to no avail.\n",
        "\n",
        "Ultimately, we determind that the best path forward was to focus on training models with improved recall scores, as that would also improve the number of individuals that were correctly being identified as members of the minority class, in this case not at risk for default and as such valuable to Home Credit's business. This was a much more successful endeavor, as we were able to create numerous models that had improved recall scores. With further feature expansion, as well as the introduction of things like interactive and polynomial terms into the modeling process, we believe we could produce some great results. Unfortunately, due to low computational power and the high computational demand of determining the ideal hyperparameters for these complex models, those theories are unable to be tested at this time."
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "oO_6dnxPXrbA"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}